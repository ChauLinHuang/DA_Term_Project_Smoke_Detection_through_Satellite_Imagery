{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DA_Term_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYxDuwDQAMLUaiWjXgYUoA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChauLinHuang/DA_Term_Project_Smoke_Detection_through_Satellite_Imagery/blob/main/DA_Term_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "446UR8YLjNnn"
      },
      "source": [
        "Import necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uiRSuzujRWs"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "from os import path, listdir\n",
        "from os.path import join, isdir, getsize\n",
        "from pathlib import Path\n",
        "from random import shuffle, seed, randrange\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sys\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# This module is manually in this script\n",
        "# import mrcnn\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "%matplotlib inline "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BLh7CeavcAk"
      },
      "source": [
        "# Import datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdb6xbd6uUzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b408ebb-2df3-4a3f-d0b8-3875c22e47f7"
      },
      "source": [
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCJMT4NlmyEK"
      },
      "source": [
        "dataset_dir = '/content/gdrive/My Drive/RPI_ITWS/Fall 2020/Data Analytics/Project/datasets/smoke_labeled/'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "502Jrn7uj7kZ"
      },
      "source": [
        "# Image data exploration\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZHgPF62j9va"
      },
      "source": [
        "print('# File sizes')\n",
        "for imageFile in listdir(dataset_dir):\n",
        "  imageDir = join(dataset_dir, imageFile)\n",
        "  imageSize = getsize(imageDir) / 1000000\n",
        "  print(imageFile + '\\t\\t' + str(round(imageSize, 2)) + 'MB' )\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IWUBoh7yn_f",
        "outputId": "998bdfb2-4a51-433d-92d0-b589806cc10c"
      },
      "source": [
        "allImgSizes = []\n",
        "for imageFile in listdir(dataset_dir):\n",
        "  imageDir = join(dataset_dir, imageFile)\n",
        "  imageSize = getsize(imageDir) / 1000000\n",
        "  allImgSizes.append(imageSize)\n",
        "    \n",
        "# Total memories\n",
        "print('Total image memories: ', round(sum(allImgSizes), 3), ' MB')\n",
        "\n",
        "# Max\n",
        "print('Image with max memory with : ', round(max(allImgSizes), 3), ' MB')\n",
        "\n",
        "# Min\n",
        "print('Image with min memory with : ', min(allImgSizes), ' MB')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total image memories:  228.376  MB\n",
            "Image with max memory with :  19.245  MB\n",
            "Image with min memory with :  0.000266  MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRcuH6YEA-h5"
      },
      "source": [
        "# Methods to bind pairs of raw and tagged images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnDbN6joRVIy"
      },
      "source": [
        "class ImagePair:\n",
        "  def __init__(self, dataset_dir: str):\n",
        "    if not path.isdir(dataset_dir):\n",
        "      raise NameError('ImagePair.__init__ exception: {} does not exist'.format(dataset_dir))\n",
        "\n",
        "    self._all_images = listdir(dataset_dir)\n",
        "    self._numImgs = len(self._all_images)\n",
        "    self._imgPairList = []\n",
        "  \n",
        "  def pair(self):\n",
        "    bmp_files = set()\n",
        "    tif_files = set()\n",
        "    for imageFile in self._all_images:\n",
        "      baseName = Path(imageFile).stem\n",
        "      if imageFile.endswith('.bmp'):\n",
        "        bmp_files.add(baseName)\n",
        "      elif imageFile.endswith('.tif'):\n",
        "        tif_files.add(baseName)\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "    matched = bmp_files.intersection(tif_files)\n",
        "\n",
        "    for baseName in matched:\n",
        "      bmp_f = baseName + '.bmp'\n",
        "      tif_f = baseName + '.tif'\n",
        "      json_f = baseName + '.json'\n",
        "      self._imgPairList.append( (bmp_f, tif_f, json_f) )\n",
        "\n",
        "  def getPairs(self) -> list():\n",
        "    return self._imgPairList\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRcANmv29CRa"
      },
      "source": [
        "# Train test split\n",
        "\n",
        "Let's use a 80-20 split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BcxlAfoIcGz"
      },
      "source": [
        "def split_to_train_test_imgs (dataset_dir, train_size = None, test_size = None):\n",
        "  '''\n",
        "  @brief: Manually split the single dataset into a train and a test dataset.\n",
        "  @param train_size: Should be a float number beteen 0 and 1 which represents the\n",
        "          the train set size percentage in the original dataset\n",
        "  @param test_size: Should be a float number between 0 and 1 which stands for the \n",
        "          test set size percentage in the original dataset\n",
        "  @return train_set: first component in the tuple\n",
        "  @return test_set: second component in the tuple\n",
        "  '''\n",
        "  if train_size is None and test_size is None:\n",
        "    train_size, test_size = 0.5, 0.5\n",
        "  elif train_size is None and test_size is not None:\n",
        "    train_size = 1 - test_size\n",
        "  elif test_size is None and train_size is not None:\n",
        "    test_size = 1 - train_size\n",
        "  \n",
        "  if train_size + test_size != 1:\n",
        "    raise ValueError('train_size and test_size do not come in reasonable ratio. Please make sure they agree with each other.')\n",
        "\n",
        "  matcher = ImagePair(dataset_dir)\n",
        "  matcher.pair()\n",
        "  image_pairs = matcher.getPairs()\n",
        "\n",
        "  total_num = len( image_pairs )\n",
        "  train_num = int( train_size * total_num ) # Note: Maybe ceil() could be a better fit?\n",
        "\n",
        "  return image_pairs[:train_num], image_pairs[train_num:]\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9DOl8psB6Rv",
        "outputId": "4520afaf-ff13-40fc-d2d5-fa9af7dda62b"
      },
      "source": [
        "# Driver code\n",
        "train_set, test_set = split_to_train_test_imgs(dataset_dir, train_size=0.8)\n",
        "\n",
        "print(train_set[0][0])\n",
        "\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "281\n",
            "71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkeer75IkM8i"
      },
      "source": [
        "# Cross-validation\n",
        "\n",
        "We are using Stratified K-Fold approach to distribute image pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nZAcgRN1jcS"
      },
      "source": [
        "class KFold_cross_validation:\n",
        "  '''\n",
        "  Prepare the splitted datasets using stratified\n",
        "  k-fold cross validation.\n",
        "\n",
        "  @return folds: is a 2D list. The rows represent the folds, each fold contains\n",
        "         the corresponding amount of image pairs.\n",
        "\n",
        "  TODO: replace Python's list with np.array or pd.DataFrame\n",
        "  '''\n",
        "  def __init__(self, imagePairs: list(), num_folds = None, _do_stratify = None):\n",
        "    self._imagePairs = list(imagePairs)\n",
        "    self._num_pairs = len( self._imagePairs )\n",
        "    if num_folds is None:\n",
        "      self._folds = 10\n",
        "    else:\n",
        "      self._folds = num_folds\n",
        "    if _do_stratify is None:\n",
        "      self._do_stratify = True\n",
        "    else:\n",
        "      self._do_stratify = _do_stratify\n",
        "    self._splitted = []\n",
        "  \n",
        "  def set_folds(self, folds: int) -> None:\n",
        "    self._folds = folds\n",
        "  \n",
        "  def set_stratify(self, do_stratify: bool) -> None:\n",
        "    self._do_stratify = do_stratify\n",
        "\n",
        "  def shuffle(self, imagePairs = None) -> None:\n",
        "    if imagePairs is None:\n",
        "      shuffle(self._imagePairs)\n",
        "    else:\n",
        "      shuffle(imagePairs)\n",
        "\n",
        "  def split(self, folds = None, do_stratify = None):\n",
        "    if folds is None:\n",
        "      folds = self._folds\n",
        "    if do_stratify is None:\n",
        "      do_stratify = self._do_stratify\n",
        "\n",
        "    fold_size = int( self._num_pairs / folds )\n",
        "\n",
        "    for i in range( folds ):\n",
        "      foldChunk = []\n",
        "      while len(foldChunk) < fold_size:\n",
        "        if do_stratify:\n",
        "          index = randrange( len(self._imagePairs) ) # use the current, smaller size of the list\n",
        "          foldChunk.append( self._imagePairs.pop(index) )\n",
        "\n",
        "        else:\n",
        "          foldChunk.append( self._imagePairs.pop() )\n",
        "      self._splitted.append(foldChunk)\n",
        "    \n",
        "    return self._splitted\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noN8pEoIM-hF"
      },
      "source": [
        "We will do cross validation across 5 folds of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxOTQP4SJOu8",
        "outputId": "8bc5af1c-1c3d-4f24-c0a2-4928dd3869ad"
      },
      "source": [
        "# driver code\n",
        "train_skf = KFold_cross_validation(train_set, num_folds = 5)\n",
        "train_skf.shuffle()\n",
        "train_folds = train_skf.split()\n",
        "\n",
        "test_skf = KFold_cross_validation(test_set, num_folds = 5)\n",
        "test_skf.shuffle()\n",
        "test_folds = test_skf.split()\n",
        "\n",
        "print('There are {} train folds each with {} image pairs.'.format( len(train_folds), len(train_folds[0]) ))\n",
        "print('There are {} test folds each with {} image pairs.'.format( len(test_folds), len(test_folds[0]) ))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 5 train folds each with 56 image pairs.\n",
            "There are 5 test folds each with 14 image pairs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTnT0dguM2tP"
      },
      "source": [
        "Now we have the trin_X, test_X, train_y, and test_y for all 5 folds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV86sHKxoWC6"
      },
      "source": [
        "# Mask R-CNN\n",
        "\n",
        "We will train and use Mask R-CNN to segment out the smoke in the images. \n",
        "The following bash script snippet does not need to be run if this is not the your first time running this Jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkqrDfN_IzqP",
        "outputId": "cc4dab13-9074-4ee9-ba7f-0a54b48ddf57"
      },
      "source": [
        "!pip install opencv-contrib-python"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm2ffWj8gXtu"
      },
      "source": [
        "We are using the Mask-RCNN repo compatible with TF 2.0. Eventhough most tutorials exemplify the package's application with the TF1.0-compatible counterpart, the codes for both version of Mask R-CNN are mostly the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qNjxQjjoVbI"
      },
      "source": [
        "%%bash\n",
        "# NOTE: Don't run the script below if you've already done it.\n",
        "# Install the ahmedfgad's version of Mask R-CNN\n",
        "git clone https://github.com/ahmedfgad/Mask-RCNN-TF2.git\n",
        "cd /content/Mask-RCNN-TF2\n",
        "pip3 install -r requirements.txt\n",
        "python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frQblhNZqoia"
      },
      "source": [
        "Test if Mask R-CNN is properly installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdkNTWo_qn9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb125e59-e962-4890-e805-ffd61b7349f5"
      },
      "source": [
        "# This is a dirty way of importing custom modules\n",
        "Mask_RCNN_repo_dir = '/content/Mask-RCNN-TF2'\n",
        "sys.path.append(Mask_RCNN_repo_dir)\n",
        "\n",
        "\n",
        "print('The Mask R-CNN modules can be imported')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Mask R-CNN modules can be imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBQKHGCaniWB"
      },
      "source": [
        "import mrcnn\n",
        "\n",
        "# Import Mask RCNN\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils, visualize\n",
        "# import mrcnn.model as modellib, log\n",
        "from mrcnn.model import log\n",
        "import mrcnn.model as modellib"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1SmawrysUwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75cffa24-4bee-4a87-fb0d-58c675c6994c"
      },
      "source": [
        "# Root directory of the project\n",
        "ROOT_DIR = path.abspath('/content/')\n",
        "\n",
        "# Directory to save logs and trained model's parameters\n",
        "MODEL_ARTIFACT_DIR = join(ROOT_DIR, \"mask_rcnn_logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "SMOKE_MODEL_PATH = join(ROOT_DIR, \"mask_rcnn_smoke.h5\")\n",
        "COCO_MODEL_PATH = join(ROOT_DIR, 'mask_rcnn_coco.h5')\n",
        "\n",
        "# Download COCO trained weights from Release if needed\n",
        "if not path.exists(COCO_MODEL_PATH):\n",
        "  utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading pretrained model to /content/mask_rcnn_coco.h5 ...\n",
            "... done downloading pretrained model!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpN2VOS2g5YW"
      },
      "source": [
        "# Create adequate configurations for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1y7tZg8ckO6"
      },
      "source": [
        "class SmokeDetectionConfig (Config):\n",
        "\t# give the configuration a recognizable name\n",
        "\tNAME = \"smoke_detection\"\n",
        "\t# set the number of GPUs to use along with the number of images\n",
        "\t# per GPU\n",
        "\tGPU_COUNT = 1\n",
        "\tIMAGES_PER_GPU = 1\n",
        "\t# number of classes (we would normally add +1 for the background\n",
        "\t# but the background class is *already* included in the class\n",
        "\t# names)\n",
        "\tNUM_CLASSES = 1 + 1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEI-TpqdcI-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c609620d-f88a-4f4a-8148-3b19a9f21fdf"
      },
      "source": [
        "# initialize the inference configuration\n",
        "config = SmokeDetectionConfig()\n",
        "config.display()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.7\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  1024\n",
            "IMAGE_META_SIZE                14\n",
            "IMAGE_MIN_DIM                  800\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [1024 1024    3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           smoke_detection\n",
            "NUM_CLASSES                    2\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                1000\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxb7TUDz6UBG"
      },
      "source": [
        "# Load in Dataset\n",
        "\n",
        "Create the dataset loader and override three methods:\n",
        "- load_dataset()\n",
        "- load_mask()\n",
        "- extract_mask()\n",
        "- image_reference()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VbnskHL7fv4"
      },
      "source": [
        "class SmokeDataset (utils.Dataset):\n",
        "  def load_dataset(self, dataset_dir):\n",
        "    self.add_class('dataset', 1, 'smoke')\n",
        "\n",
        "    # find all raw images\n",
        "    _raw_format = '.tif'\n",
        "    for i, filename in enumerate( listdir(dataset_dir) ):\n",
        "      if filename.endswith(_raw_format):\n",
        "        self.add_image('dataset',\n",
        "                       image_id=i,\n",
        "                       path=join(dataset_dir, filename),\n",
        "                       annotation=join(dataset_dir, filename.replace(_raw_format, '.xml')))\n",
        "    \n",
        "  # extract bounding boxes\n",
        "  # TODO\n",
        "  def extract_boxes(self, filename):\n",
        "    '''\n",
        "    Since I don't have the bounding box labels, I don't have any tagged images\n",
        "    for this category. Gotta leave it blank for the moment. Will add this feature\n",
        "    back later.\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "  # Extract custom instance segmentations\n",
        "  def extract_mask(self, filename):\n",
        "    json_file = join(filename)\n",
        "    with open(json_file) as f:\n",
        "        img_anns = json.load(f)\n",
        "        \n",
        "    masks = np.zeros([600, 800, len(img_anns['shapes'])], dtype='uint8')\n",
        "    classes = []\n",
        "    for i, anno in enumerate(img_anns['shapes']):\n",
        "        mask = np.zeros([600, 800], dtype=np.uint8)\n",
        "        cv2.fillPoly(mask, np.array([anno['points']], dtype=np.int32), 1)\n",
        "        masks[:, :, i] = mask\n",
        "        classes.append(self.class_names.index(anno['label']))\n",
        "    return masks, classes\n",
        "\n",
        "\n",
        "  # Load the masks for an image\n",
        "  def load_mask(self, image_id):\n",
        "    # get details of image\n",
        "    info = self.image_info[image_id]\n",
        "    # define box file location\n",
        "    path = info['annotation']\n",
        "    # load XML\n",
        "    boxes, classes, w, h = self.extract_boxes(path)\n",
        "    # create one array for all masks, each on a different channel\n",
        "    # WIP: Maybe we only need one channel, since its black and white\n",
        "    num_boxes = len(boxes)\n",
        "    masks = np.zeros([h, w, num_boxes], dtype = 'uint8')\n",
        "\n",
        "    # create masks\n",
        "    for i in range(num_boxes):\n",
        "      box = boxes[i]\n",
        "      row_s, row_e = box[1], box[3]\n",
        "      col_s, col_e = box[0], box[2]\n",
        "      masks[row_s : row_e, col_s : col_e, i] = 1\n",
        "\n",
        "    return masks, np.asarray(classes, dtype='int32')\n",
        "  \n",
        "  def image_reference (self, image_id):\n",
        "    info = self.image_info[image_id]\n",
        "    return info['path']"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3RaWwFaVD6v"
      },
      "source": [
        "### Significant variables:\n",
        "train_dataset \\\n",
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU9X5C4jHtgr",
        "outputId": "9fad2d58-2167-445c-92c2-162baf4e033e"
      },
      "source": [
        "# Create training and validation dataset\n",
        "# train set\n",
        "train_dataset_dir = '/content/gdrive/MyDrive/RPI_ITWS/Fall 2020/Data Analytics/Project/datasets/train'\n",
        "train_dataset = SmokeDataset()\n",
        "train_dataset.load_dataset(train_dataset_dir)\n",
        "train_dataset.prepare()\n",
        "\n",
        "print('Train dataset: ', len(train_dataset.image_ids), ' samples.')\n",
        "\n",
        "# test or validation set\n",
        "test_dataset_dir = '/content/gdrive/MyDrive/RPI_ITWS/Fall 2020/Data Analytics/Project/datasets/test'\n",
        "val_dataset = SmokeDataset()\n",
        "val_dataset.load_dataset(test_dataset_dir)\n",
        "val_dataset.prepare()\n",
        "\n",
        "print('Test dataset: ', len(val_dataset.image_ids), ' samples.')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset:  311  samples.\n",
            "Test dataset:  71  samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "TPxXNr-1J3ij",
        "outputId": "eace47ed-ee22-4e33-efbe-2e6e05cf8bab"
      },
      "source": [
        "# Load and display random samplers\n",
        "num_features = 1\n",
        "image_ids = np.random.choice(train_dataset.image_ids, num_features)\n",
        "\n",
        "for image_id in image_ids:\n",
        "  image = train_dataset.load_image(image_id)\n",
        "  mask, class_ids = train_dataset.load_mask(image_id)\n",
        "  visualize.display_top_masks(image, mask, class_ids, dataset_train_class_names)\n",
        "  "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0d840444432f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mvisualize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_top_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_class_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-af346a256eb8>\u001b[0m in \u001b[0;36mload_mask\u001b[0;34m(self, image_id)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# load XML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;31m# create one array for all masks, each on a different channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# WIP: Maybe we only need one channel, since its black and white\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDn-Rm_AX17y"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBqmk8VFVLw-"
      },
      "source": [
        "### Significant variables\n",
        "model \\\n",
        "init_with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vleIfxrtXNex"
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode='training',\n",
        "                          config=config,\n",
        "                          model_dir=MODEL_ARTIFACT_DIR)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6XV5g8mUGj3"
      },
      "source": [
        "# which weight to start with?\n",
        "init_with = 'coco' # imagenet, coco, or last\n",
        "\n",
        "if init_with == 'imagenet':\n",
        "  model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_with == 'coco':\n",
        "  '''Load weights trained on MS COCO, but skip layers that\n",
        "    are different due to the different number of classes'''\n",
        "  model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_with == \"last\":\n",
        "  # Load the last model you trained and continue training\n",
        "  model.load_weights(model.find_last(), by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATVyDbNRU4vT"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92bntAXtU8iG"
      },
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "model.train(train_dataset, val_dataset, \n",
        "            learning_rate=config.LEARNING_RATE, \n",
        "            epochs=5, \n",
        "            layers='heads')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFYMN8k5TtgG"
      },
      "source": [
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print('Loading weights from ', model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8qwzceH7bO_"
      },
      "source": [
        "# Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE / 10,\n",
        "            epochs=10, \n",
        "            layers=\"all\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXrj8wYT7lHI"
      },
      "source": [
        "# Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-NbgMi87mYn"
      },
      "source": [
        "class InferenceConfig(SmokeDetectionConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wviE18Tr77Q-"
      },
      "source": [
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_ARTIFACT_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3wYIcGDNRPq"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h441iwh3WP6z"
      },
      "source": [
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lalBqEkG8Zqy"
      },
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(val_dataset.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(val_dataset, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgCdoClJ9ZYX"
      },
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKx43wpU9W9f"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Metrics: Average Precision (AP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18cg-Ltl-c6k"
      },
      "source": [
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = val_dataset.image_ids\n",
        "APs = []\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "    \n",
        "print(\"mAP: \", np.mean(APs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZIAwYsxm7zu"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Initial EDA - Image Processing | Kaggle \\\n",
        "https://www.kaggle.com/cc786537662/initial-eda-image-processing\n",
        "\n",
        "[2] TannerGilbert/MaskRCNN-Object-Detection-and-Segmentation: Train your own custom MaskRCNN Object Detection and Instance Segmentation model. \\\n",
        "https://github.com/TannerGilbert/MaskRCNN-Object-Detection-and-Segmentation"
      ]
    }
  ]
}